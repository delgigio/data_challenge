---
title: "Stroke analysis and prediction"
output: html_notebook
---

# Title
### Background
We are a medical research institute which collaborates with many hospitals over the territory of Switzerland.
One of the hospitals recently provided us a dataset about patients with the task of analyzing the typical profiles that are more related to brain strokes. As the data contained only patients that had a stroke we asked the hospital to give us data about people who did in fact not have a stroke. This way we had a lot more data to work on and comprare to the previously-provided samples.
After discussing between us we found some other interesting analysis to be done on the dataset and we figured out it would be useful to ask data experts to help us with the analysis. On top of that, using machine learning would help us discover hidden patterns and evaluate some of the tools in the medical sectors.
Therefore we decided to come to you with a series of questions and possible analysis topics.



## Preparing the data
After loading the dataset using readr, the first step is to check whether it's tidy.

```{r}
# loading useful libraries


library(rlang)  #  pass string feature names to functions
install.packages('themis')
install.packages('randomForest')
install.packages("infer")
library(infer)
library(themis)
install.packages('performance')
library(performance) # check if models are valid
library(tidyverse)
library(glue)  # f-strings
library(tidymodels)
library(caret)  # stratified split

install.packages('corrr')
library(corrr)   # corr matrix
```

```{r}
data <- read_csv("./healthcare-dataset-stroke-data.csv", show_col_types =FALSE)
dim(data); colnames(data)
```
We have over 5100 patients and 12 columns, with 'stroke' being the binary target of the first question.
From the feature names and the first few lines we can confirm that each column represents a characteristic of the patient and no cell contains more than one values that could be split.

```{r}
head(data, 5)
```
The last check needed for a tidy dataset is that each row is a different observation, in our case a patient.
To do so we make sure that all of the people are different by counting their IDs. We can see that it's indeed the case.

```{r}
length(unique(data$id)); dim(data)[[1]]
```
From this point on, ID is not relevant anymore as completely unique values have no predictive power and aren't useful for any analysis, we will remove them.

```{r}
data <- select(data, -id)
```


Often data collection may produce mistakes or incomplete observations. Let's check the missing values of our dataset for each column.

```{r}
sapply(data, function(x) sum(x == "N/A"))
```
The only column that contains NaN values is 'bmi' and we can notice that we don't have any height or weight columns, thus it can't be extracted. There are many ways to impute this values.
For now we eliminate the rows with missing bmi.

```{r}
data <- data[!apply(data == "N/A", 1, any), ]

sapply(data, function(x) sum(x == "N/A"))

data$bmi <- as.numeric(data$bmi)
```

Before starting to visualize and analyze we should divide the dataset into train + validation and test sets.
The test set is supposed to simulate fully unknown data and won't be used until the end of the process. The train set is used to train models, and the validation set is used to evaluate models inside of the workflow and to optimize them without leaking the test data.
We believe visualizing should also be done only on the train set as we might notice some patterns that we weren't supposed to see by using the test set.

We use a stratified split from caret to keep the ratio of patients that had and didn't have a stroke in all datasets the same, this is important since we have few patients that had a stroke and they could all end up in one side after the split.

```{r}

data$stroke <- as.factor(data$stroke)

train.index <- createDataPartition(data$stroke, p = 0.75, list = FALSE)
tmp_train <- data[ train.index,]
tb_test  <- data[-train.index,]

train_2.index <- createDataPartition(tmp_train$stroke, p = 0.75, list = FALSE)
tb_val  <- tmp_train[-train_2.index,]
tb_train <- tmp_train[ train_2.index,]

print(glue("Train set shape: ({dim(tb_train)[[1]]}, {dim(tb_train)[[2]]})"))
print(glue("Validation set shape: ({dim(tb_val)[[1]]}, {dim(tb_val)[[2]]})"))
print(glue("Test set shape: ({dim(tb_test)[[1]]}, {dim(tb_test)[[2]]})"))
```

We can start doing some data exploration.
Let's start by understanding the numeric variables, we don't have many of them in this dataset as most of them are either binary or categorical. The options are age, bmi and avg_glucose_level.

```{r}
numeric_columns <- c('age','avg_glucose_level','bmi')

print(summary(select(tb_train, all_of(numeric_columns))))
```
We plot their PDFs to understand their distributions.

```{r}
for (col in numeric_columns) {
  p <- ggplot(tb_train, aes(x = !!sym(col))) +
    geom_histogram(bins = 30, fill = "blue", color = "black") +
    labs(title = glue("PDF of '{col}'"), x = col, y = "Frequency") +
    theme_minimal()
  print(p)
}
```
From the PDF of age we can see that our dataset is quite varied with many patients coming from all age ranges. Although there are a few peaks around the mean, it isn't really a normal distribution. 
Bmi on the other hand shows a well-defined bell shape centered around 29, it is slightly right-skewed and some outliers can be noticed.
The PDF of the glucose level is the most interesting one as we can notice a separation between values before 175~ and after, it looks like two normal distributions put together as the points far from the 'first bell' are quite a lot.



```{r}
for (col in numeric_columns) {
  p <- ggplot(tb_train, aes(x = factor(0), y = !!sym(col))) +
    geom_boxplot() +
    labs(title=glue('boxplot of {col}'),y = col, x = "") +
    theme_minimal()
  print(p)
}
```
Box-plots are very useful to notice outliers and are simple to make. As we discussed before, age is widely distributed and no point is really considered an outlier. The other two show many outliers on the upper side of the distribution, but since we already talked about them, and removing them when dealing with real patients may not always be a great idea, we'll keep them in.

We can now add an additional dimension to the plots in order to check the distributions compared to our target 'stroke'.

```{r}
for (col in numeric_columns) {
  p <- ggplot(tb_train, aes(x = factor(stroke), y = !!sym(col), fill = factor(stroke))) +
    geom_boxplot() +
    labs(title = glue("Box plot of {col} stratified by stroke"), y = col, x = "Stroke (0 = False, 1 = True)") +
    theme_minimal() +
    theme(legend.position = "none")
  print(p)
}
```
The boxes of bmi are almost overlapping, this probably means that the feature isn't the most relevant for the prediction, and the presence of more outliers on the left side is due to more people belonging to that group.

The other two are more interesting.
As expected, strokes seem to be the most prevalent on older people, the vast majority of the distribution is entirely above the one of 'healthy' people, and it's in the range 60-80. This is clearly a very important feature.

The glucose level plot shows that the majority of people who didn't have a stroke is contained in a subset (75-120) of the distribution of the ones who did. This signals that glucose may not be the most important factor, but higher levels of it may affect the likelihood of having a stroke.


```{r}
ggplot(tb_train, aes(x = age, y = avg_glucose_level)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "age vs avg_glucose_level", x = "age", y = "avg_glucose_level") +
  theme_minimal()
```
### comment this
### add correlation matrix


```{r}
tb_train$stroke_num <- as.numeric(tb_train$stroke)

tb_train %>% correlate()
```
From a simple correlation matrix, age, hypertension, heart_disease and avg_glucose_level seem decently correlated to the strokes, but not very much, we'll deepen our analysis later.

```{r}
tb_train <- select(tb_train, -stroke_num)
```

Now let's focus on the categorical variables.

```{r}
print(table(tb_train$gender))
cat("\n\n")
print(table(tb_train$work_type))
cat("\n\n")
print(table(tb_train$smoking_status))
cat("\n\n")
print(table(tb_train$Residence_type))
```



For the non-binary variable we show a piechart which intuitively highlights the distribution.

```{r}
work_counts <- tb_train %>%
  group_by(work_type) %>%
  summarise(count = n())

ggplot(work_counts, aes(x = "", y = count, fill = work_type)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  theme_void() +
  labs(title = "Distribution of work_Type", fill = "Work Type")


smoke_counts <- tb_train %>%
  group_by(smoking_status) %>%
  summarise(count = n())

ggplot(smoke_counts, aes(x = "", y = count, fill = smoking_status)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  theme_void() +
  labs(title = "Distribution of smoking_status", fill = "status")
```
It's interesting to note that a sizable chunk of the sample is formed by children. The large majority of patients are privates, however we don't believe this is going to be very relevant for the predictions.

The smoking status is instead directly related to health, we can see that the patients are evenly distributed between three groups: smokers (formerly smoked - smokes), non-smokers and unknown, it's unfortunate that this latter group is as big as it is since we could have had much more information. 


Next the binary variables, the target is included here.
NOTE: Gender is not binary, there is a single sample in the 'Other' category, however it isn't plotted as a pie-charts since those are great when the variable has more categories.

```{r}
ggplot(data, aes(x = factor(stroke))) +
  geom_bar(fill = c("blue", "orange")) +
  labs(title = "Distribution of Stroke", x = "Stroke", y = "Count") +
  theme_minimal()

# Bar chart for 'residence_type'
ggplot(data, aes(x = factor(Residence_type))) +
  geom_bar(fill = c("green", "red")) +
  labs(title = "Distribution of Residence Type", x = "Residence Type", y = "Count") +
  theme_minimal()

# Bar chart for 'gender'
ggplot(data, aes(x = factor(gender))) +
  geom_bar(fill = c("purple", "yellow", "green")) +
  labs(title = "Distribution of Gender", x = "Gender", y = "Count") +
  theme_minimal()

```
We can notice a basically even residence type distribution, and a decently balanced gender one, with more female patients being present in our train set.

The plot of 'stroke' makes apparent what is going to be a big issue for our models, heavily unbalanced target. It was to be expected, the majority (around 95%) of patients didn't experience a stroke.

```{r}
tb_train %>% group_by(stroke) %>%
  summarise(n=n()) %>% mutate(freq=n/sum(n))
```


```{r}
stroke_residence_gender_counts <- tb_train %>%
  group_by(stroke, Residence_type, gender) %>%
  summarise(count = n()) %>%
  ungroup()

ggplot(stroke_residence_gender_counts, aes(x = Residence_type, y = count, fill = gender)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~stroke) +
  labs(title = "Distribution of Stroke by Residence Type and Gender",
       x = "Residence Type",
       y = "Count",
       fill = "Gender") +
  theme_minimal()
```
Although the residence type doesn't seem to heavily affect the target, the gender may very well be. The distribution by gender of the negative-target group seem to reflect the actual distribution of the gender, meanwhile men in rural settings may be more subjected to strokes.

## 1) Binary Classification of Strokes
"- Is it possible to predict strokes with ML / statistical methods?"

We can now start our classification task, we will try to predict the variable 'stroke' by using all of the features, and trying to understand the most important ones.


In the healthcare sector, explainability is often as important as actual performance, clients want to know why a decision was made, after all we are dealing with a person body.

Therefore, we'll start with one of the most simple model types, a linear model. In particular we'll perform logistic regression using 'tidymodels'.

First we define recipes that will be useful for the whole task, and models workflows.

```{r}
base_recipe <- recipe(stroke ~ ., data = tb_train)

 stroke_recipe <- base_recipe %>% step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())
```

The recipe is very handy because it will normalize the data and perform One-Hot encoding on the categorical variables, without having to do it manually.

We now create our logistic regressor, without any regularization, using the glm engine.

```{r}

logreg <- logistic_reg() %>% set_engine('glm')

logreg_workflow <- workflow() %>% add_model(logreg) %>% add_recipe(stroke_recipe)

```

We can now train the model on our tb_train, we will use all the features in order to predict the variable 'strokes' (specified in the recipe).

```{r}
logreg_fit_workflow <- logreg_workflow %>% fit(tb_train)

logreg_fit_workflow
```
```{r}
logreg_fit_workflow %>% 
  extract_fit_engine %>% 
  summary()
```
From the summary we can obtain a overview of the importance of each feature.
The smaller the Pr(>|z|) column (p-value) is, the most significant the feature, basically it allows us to state if a feature is significant or not.

We can see that age is the most significant feature, indeed it has a statistic value of 8.899, and since we applied normalization we can also use this column to analyze the importance. We expected this result both from common sense and from our visualizations.

We also mentioned before that bmi was probably not going to be very relevant and it is indeed one of the three least significant features, along with being female (we said th at being male might positively influence the target, not the opposite) and having never worked (work type is in general not very significant, although being self employed has the lowest p-value among them, potentially due to high stress).


We evaluate the model on our validation set by augmenting it with the predictions and their probabilities.

```{r}
logreg_valPred <- logreg_fit_workflow %>% augment(tb_val)
logreg_valPred %>% head(1)
```
Let's evaluate the model, the most used metric for classification is accuracy.

```{r}
yardstick::metrics(logreg_valPred, truth=stroke, estimate=.pred_class)
```
An accuracy of 95% is good, but also suspicious, indeed we mentioned that we were dealing with unbalanced classes. Accuracy is a poor choice in this situations, a model could reach high accuracy by always predicting the negative class.

```{r}
logreg_valPred %>% group_by(stroke) %>%
  summarise(n=n()) %>% mutate(freq=n/sum(n))
```
A better way to evaluate a classifier is with a confusion matrix.

```{r}
logreg_cm <- logreg_valPred %>%
  conf_mat(truth=stroke,estimate=.pred_class)

logreg_cm
```
While we obtain a good accuracy, our model is never predicting the positive class.

It's now a good time to talk about the main metric we'll try to optimize in this project, the recall. It is defined as 
TP / (TP + FN), maximizing it means minimizing the false negatives, something key in this case. A patient being labeled 'safe' having a stroke is a much worse error than a safe patient being labeled 'at risk'; in the second case we might loose some money, in the first one potentially a life.

```{r}
yardstick::recall(logreg_valPred, truth=stroke,estimate=.pred_class,event_level='second')
```
Since the model never predicts stroke=1, its recall is 0. 
Since we are minimizing recall, we can find the best threshold for our models using ROC curves.
These are plots that have the false positive rate (FPR) as x-axis and the true positive rate(recall/TPR) as y-axis. Each point represents a different threshold for the target separation. To measure how good a model is we can use the area under the ROC curve.

```{r}
plot_roc_curve <- function(prediction_dataset, target) {
auroc <- yardstick::roc_auc(prediction_dataset, {{ target }},.pred_1,event_level = 'second')$.estimate

roc_curve(prediction_dataset, {{ target }},.pred_1,event_level = 'second') %>% autoplot() +   geom_label(aes(x = 0.75, y = 0.25, label = paste("AUROC:", round(auroc, 5))), size = 4, label.size = 0.5)
  
}
```

```{r}
plot_roc_curve(logreg_valPred, 'stroke')
```

We obtain a decent AUROC, and we could decide to extract a threshold from the curve, but we'll try more models to see different perspectives.

Next we'll try a more powerful, but also less explainable model, a Random Forest. This is done to see if by using a strong model we can significantly improve our score.

```{r}
rand_forest_spec <- rand_forest(mtry = 3) %>%
  set_engine('randomForest') %>%
  set_mode('classification')
```

```{r}

rf <- rand_forest(mtry = 3) %>%
  set_engine('randomForest') %>%
  set_mode('classification')


 rf_workflow <- workflow() %>% add_model(rf) %>% add_recipe(stroke_recipe)


rf_fit_workflow <- rf_workflow %>% fit(tb_train)
```



```{r}
rf_valPred <- rf_fit_workflow %>% augment(tb_val)
rf_valPred %>% head(1)
```



```{r}
rf_cm <- rf_valPred %>%
  conf_mat(truth=stroke,estimate=.pred_class)

rf_cm

yardstick::recall(rf_valPred, truth=stroke,estimate=.pred_class,event_level='second')
```
```{r}
plot_roc_curve(rf_valPred, 'stroke')
```

We obtain a bad performance here too. If a powerful model can't really learn to distinguish the classes it's a sign that we need to address the imbalance.  Our AUROC are also very similar.

Typical techniques to do so include oversampling of the least-common class, SMOTE or our choice, undersampling of the most-common class. This means sampling N (or close to it) patients that didn't experience a stroke, where N is the numbers of those who did. Although we loose a lot of information, we are not introducing dummy observations.


```{r}
us_stroke_recipe <- stroke_recipe %>% step_downsample(stroke)
```


```{r}
logreg <- logistic_reg() %>% set_engine('glm')

us_logreg_workflow <- workflow() %>% add_model(logreg) %>% add_recipe(us_stroke_recipe)

us_logreg_fit_workflow <- us_logreg_workflow %>% fit(tb_train)

us_logreg_fit_workflow
```

```{r}
us_logreg_valPred <- us_logreg_fit_workflow %>% augment(tb_val)
us_logreg_valPred %>% head(1)
```
```{r}
us_logreg_cm <- us_logreg_valPred %>%
  conf_mat(truth=stroke,estimate=.pred_class)

us_logreg_cm
```
```{r}
#us_logreg_trainPred <- us_logreg_fit_workflow %>% augment(tb_train)


#us_logreg_train_cm <- us_logreg_trainPred %>%
 # conf_mat(truth=stroke,estimate=.pred_class)

#us_logreg_train_cm
```


```{r}
yardstick::recall(us_logreg_valPred, truth=stroke,estimate=.pred_class,event_level='second')
```
Our new classifier has a much lower accuracy, but a infinitely higher recall. This is what we wanted, we now check the AUROC of the model.

```{r}
plot_roc_curve(us_logreg_valPred, 'stroke')
```
```{r}
us_rf_workflow <- workflow() %>% add_model(rf) %>% add_recipe(us_stroke_recipe)


us_rf_fit_workflow <- us_rf_workflow %>% fit(tb_train)
```




```{r}
us_rf_valPred <- us_rf_fit_workflow %>% augment(tb_val)
us_rf_valPred %>% head(1)
```



```{r}
us_rf_cm <- us_rf_valPred %>%
  conf_mat(truth=stroke,estimate=.pred_class)

print(us_rf_cm)

yardstick::recall(us_rf_valPred, truth=stroke,estimate=.pred_class,event_level='second')
```


```{r}
plot_roc_curve(us_rf_valPred, 'stroke')
```

##################






```{r}
logreg_lasso <- logistic_reg(penalty = 39, mixture = 1) %>%
  set_engine('glmnet')

# Create the workflow with the logistic regression model and recipe
logreg_lasso_workflow <- workflow() %>%
  add_model(logreg_lasso) %>%
  add_recipe(stroke_recipe)

# Fit the workflow to the training data
logreg_lasso_fit_workflow <- logreg_lasso_workflow %>%
  fit(tb_train)

# Check the fitted workflow
logreg_lasso_fit_workflow
```


```{r}
logreg_ridge <- logistic_reg(penalty = 1, mixture = 0) %>%
  set_engine('glmnet')

# Create the workflow with the logistic regression model and recipe
logreg_ridge_workflow <- workflow() %>%
  add_model(logreg_ridge) %>%
  add_recipe(stroke_recipe)

# Fit the workflow to the training data
logreg_ridge_fit_workflow <- logreg_ridge_workflow %>%
  fit(data = tb_train)

# Print the fitted workflow to ensure the output is visible
print(logreg_ridge_fit_workflow)
```


```{r}
evaluate_model <- function(fit_workflow, validation_set) {
  # Perform predictions on the validation set
  val_predictions <- fit_workflow %>%
    augment(new_data = validation_set)

  # Display the first 300 predictions
  print(head(val_predictions, 300))

  # Calculate and print evaluation metrics
  metrics_results <- yardstick::metrics(val_predictions, truth = stroke, estimate = .pred_class)
  print(metrics_results)

  # Generate and print confusion matrix
  cm <- val_predictions %>%
    conf_mat(truth = stroke, estimate = .pred_class)
  print(cm)

  # Calculate and print recall
  recall_value <- yardstick::recall(val_predictions, truth = stroke, estimate = .pred_class, event_level = 'second')
  print(recall_value)

  # Plot the ROC curve
  if (any(names(val_predictions) == ".pred_1")) {
    roc_data <- roc_curve(val_predictions, truth = stroke, .pred_1, event_level = 'second')
  } else if (any(names(val_predictions) == ".pred_0")) {
    roc_data <- roc_curve(val_predictions, truth = stroke, .pred_0, event_level = 'second')
  } else {
    stop("Predicted probability column not found.")
  }

  roc_plot <- autoplot(roc_data)
  print(roc_plot)
}

```



```{r}
evaluate_model(rf_fit_workflow, tb_val)
```



```{r}
evaluate_model(logreg_lasso_fit_workflow, tb_val)

```

```{r}
evaluate_model(logreg_ridge_fit_workflow, tb_val)

```


### *) Regression on Age
During our previous analysis we found out that 'age' is the most important feature to predict 'stroke'. Not only that, we found that it was by far the most significant one.

A reason for this could be that some of the other features don't add much information once we already have the age. A way to check this is by looking at the correlation between the features,  if they bring very different information they shouldn't be highly-correlated. After these consideration we build a regression model to predict age given the other features.

```{r}
tb_train_no_stroke <- select(tb_train, -stroke)
tb_val_no_stroke <- select(tb_val, -stroke)


age_recipe_base <- recipe(age ~ ., data = tb_train_no_stroke)


 age_recipe <- age_recipe_base %>% step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

 
linreg <-  linear_reg() %>% set_engine("lm")
  
linreg_workflow <- workflow() %>% add_model(linreg) %>% add_recipe(age_recipe)
 
linreg_fit_workflow <- linreg_workflow %>% fit(tb_train_no_stroke)
```

```{r}
linreg_fit_workflow %>% extract_fit_engine() %>% summary()
```
We notice something that we were expecting,  almost all of the other features have a high correlation with the age.
Some of these are intuitive, in particular those related to the health of a patient such as hypertension, heart_disease, avg_glucose_level; others are harder to explain but since they refer to experiences (marriages, smoking history, jobs...) they could explain something about the age, an older patient is more likely to have been part of a marriage than a younger one. 

Although this was the main goal we had, we can still try to evaluate the regressor to determine how much of age is explained by the features.

```{r}
linreg_trainPred <- linreg_fit_workflow %>% augment(tb_train_no_stroke)

bind_rows(yardstick::rsq(linreg_trainPred, truth=age,estimate=.pred), yardstick::rmse(linreg_trainPred, truth=age,estimate=.pred))
```

```{r}
linreg_valPred <- linreg_fit_workflow %>% augment(tb_val_no_stroke)

bind_rows(yardstick::rsq(linreg_valPred, truth=age,estimate=.pred), yardstick::rmse(linreg_valPred, truth=age,estimate=.pred))
```
We can see that the R^2 of the model is pretty consistent both on the train and validation set, around 0.66, meaning that the features explain around 66% of the age variable. This confirms our ideas: while of course the features alone aren't able to perfectly predict the age, there is definitely correlation among them.

To consider this a valid analysis we first need to be sure that that we are using a valid model.


##############
```{r}
linreg_fit_workflow %>% 
  extract_fit_engine() %>%  
  check_model()
```


```{r}
train_withResid <- linreg_fit_workflow %>%
  extract_fit_engine() %>%
  augment()

train_withResid %>% 
  ggplot()+ 
  geom_point(aes(
    x=bmi,
    y=.resid))

# for each feature

```




### *) Hypothesis testing on mean BMI between target classes

While we are sure there is a significant difference between the mean age of people who had a stroke and people who hadn't, the other two numeric features were more interesting.

We can check if two means are significantly different with hypothesis testing.

μS : mean bmi of people who had a stroke.
μNS : mean bmi of people who didn't have a stroke.


H0 :  μS =  μNS
H1 :  μS != μNS

```{r}
mean_bmi <- tb_train %>% 
  summarize(mean(bmi)) %>% pull(); 
sd_bmi <- tb_train %>% 
  summarize(sd(bmi)) %>% pull()


simulatedGauss <- 
  tibble(bmi=rnorm(n = 1000,
                        mean=mean_bmi,
                        sd = sd_bmi))

combined_bmi <- bind_rows(tb_train %>% select(bmi) %>% mutate(source='real'), simulatedGauss %>% mutate(source='simulation'))


ggplot(combined_bmi, aes(x = bmi, color = source)) +
  geom_density() +
  labs(title = "distribution of real BMI  and simulated gaussian",
       x = "BMI",
       y = "Density") +
  theme_minimal()
```
The plot is useful to check the normality of our data. Although we already saw the PDF of it, we can also compare it to the simulated gaussian data using its parameters. The two curves are similar, thus we can say that BMI is approximately normally distributed and we can do a t-test.



```{r}
bmi_stroke <- tb_train %>% filter(stroke==1) %>% select(bmi)

bmi_safe <- tb_train %>% filter(stroke==0) %>% select(bmi)

print(glue("Mean BMI of people who didn't have a stroke: {mean(bmi_stroke$bmi)}"))

print(glue("Mean BMI of people who had a stroke: {mean(bmi_safe$bmi)}"))
```


```{r}
t.test(bmi_stroke$bmi, bmi_safe$bmi)
```
The p-value is smaller than 0.05 (significance), meaning that we can reject H0 and state that the difference between two means is statistically significant, even if the means themselves seem relatively close. The estimated difference range from 0.309 to 2.657. 


To confirm this result we can use a different approach, here we'll use a permutation test.


```{r}
null_hypothesis <- tb_train %>%  specify(response = bmi, explanatory = stroke) %>%  hypothesize(null = "independence")

null_distribution <- null_hypothesis %>%  generate(reps = 1000, type = "permute") %>%  calculate(stat = "diff in means", order = c(0, 1))

```

```{r}
observed_stat <- tb_train %>%  specify(response = bmi, explanatory = stroke) %>%  calculate(stat = "diff in means", order = c(0, 1))

null_distribution %>%  visualize() +  shade_p_value(obs_stat = observed_stat, direction = "two-sided")

p_value <- null_distribution %>%  get_p_value(obs_stat = observed_stat, direction = "two-sided")

p_value
```

### *) ANOVA: BMI between work types

```{r}
tb_train %>%  group_by(work_type) %>% summarise(mean=mean(avg_glucose_level), sd=sd(avg_glucose_level),n=n())
```
```{r}
tb_train %>% ggplot()+geom_boxplot(aes(x=work_type, y=bmi))
```

Three classes (Govt_job, Private and Self-employed) seem to have very similar distributions, with very similar means and standard deviations. The children group is instead isolated from the other with smaller parameters.

To obtain an actual statistical answer to the question we'll utilize ANOVA. In simple words, we'll compare the mean of all of these groups and test the hypothesis:

H0 : all means are equal.
H1 : at least one mean is different.

```{r}
observed_f_statistic <- tb_train %>%  specify(bmi~work_type) %>%  hypothesize(null = "independence") %>%  calculate(stat = "F")

null_dist <- tb_train %>%  specify(bmi~work_type) %>%  hypothesize(null = "independence") %>%  generate(reps = 1000, type = "permute") %>%  calculate(stat = "F")
```


```{r}
null_dist %>%  visualize() +   shade_p_value(observed_f_statistic, direction = "greater")
```


```{r}
null_dist %>%  get_p_value(obs_stat = observed_f_statistic, direction = "greater")
```
We can reject H0 with extremely high confidence. 
Now we can try to find the different group(s) and repeat the test.

```{r}
tb_train_review <- tb_train %>% filter(work_type != "children")
```


```{r}
observed_f_statistic <- tb_train_review %>%  specify(bmi~work_type) %>%  hypothesize(null = "independence") %>%  calculate(stat = "F")
null_dist <- tb_train_review %>%  specify(bmi~work_type) %>%  hypothesize(null = "independence") %>%  generate(reps = 1000, type = "permute") %>%  calculate(stat = "F")
```


```{r}
null_dist %>%  visualize() +   shade_p_value(observed_f_statistic, direction = "greater")
```

```{r}
null_dist %>%  get_p_value(obs_stat = observed_f_statistic, direction = "greater")
```
Indeed now the p-value is quite big. The children category is the one significantly different from the other, it intuitively makes sense, children have often lower BMI values due to several reasons like higher metabolic rate (they burn more calories) and higher growth rate (muscles and bone density rapidly increase, possibly more than weight), and hormones effects.

### *) Age range at most risk





```{r}
age_stroke <- tb_train %>% filter(stroke == 1) %>% select(age)

mean_age_stroke <- mean(age_stroke[["age"]])

```

```{r}

booStrap_age <- tb_train %>%    specify(response=age) %>%  generate(reps = 1000, type="bootstrap") %>%   calculate(stat="mean")

CI_age <- booStrap_age %>%   get_confidence_interval(point_estimate = mean_age_stroke, level = .95, type='percentile')

CI_age
```

```{r}
booStrap_age %>% visualize() + shade_confidence_interval(endpoints = CI_age)+  geom_vline(xintercept = mean_age_stroke, linetype = "dashed")
```
